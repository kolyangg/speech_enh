{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import Dataset, Audio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True, attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n",
    "# sample = dataset[0][\"audio\"]\n",
    "\n",
    "# result = asr_pipeline(sample,return_timestamps=True)\n",
    "# print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 824/824 [00:14<00:00, 55.28 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "# Path to your folder containing WAV files\n",
    "audio_folder = \"data/asr_testing/clean\"\n",
    "\n",
    "# Build a list of file paths for the WAV files\n",
    "file_paths = [\n",
    "    os.path.join(audio_folder, fname)\n",
    "    for fname in os.listdir(audio_folder)\n",
    "    if fname.lower().endswith(\".wav\")\n",
    "]\n",
    "\n",
    "\n",
    "# Create a dataset from the list of file paths.\n",
    "# Here, the column \"file\" will initially hold the file path (a string).\n",
    "dataset = Dataset.from_dict({\"file\": file_paths})\n",
    "\n",
    "# Cast the \"file\" column to the Audio feature. This converts the file path to a dict with keys:\n",
    "# \"array\", \"sampling_rate\", and \"path\".\n",
    "dataset = dataset.cast_column(\"file\", Audio())\n",
    "\n",
    "# Add a filename column to each sample (extracted from the file's path)\n",
    "def add_filename(example):\n",
    "    example[\"filename\"] = os.path.basename(example[\"file\"][\"path\"])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_filename)\n",
    "\n",
    "# Define a batched transcription function.\n",
    "def transcribe(batch):\n",
    "    # Instead of passing file paths, pass the actual audio arrays\n",
    "    audio_arrays = [x[\"array\"] for x in batch[\"file\"]]\n",
    "    # Transcribe the batch using the pipeline.\n",
    "    results = asr_pipeline(audio_arrays)\n",
    "    texts = [res[\"text\"] for res in results]\n",
    "    return {\"transcription\": texts}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 824/824 [05:20<00:00,  2.57 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcriptions have been saved to asr_output.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the transcription function using batched mapping.\n",
    "result_dataset = dataset.map(transcribe, batched=True, batch_size=6)\n",
    "\n",
    "# Build a dictionary mapping filenames to transcriptions.\n",
    "transcriptions = {\n",
    "    entry[\"filename\"]: entry[\"transcription\"] for entry in result_dataset\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcriptions have been saved to asr_output.json\n"
     ]
    }
   ],
   "source": [
    "# sort by filename\n",
    "transcriptions = dict(sorted(transcriptions.items()))\n",
    "\n",
    "# Save the dictionary to a JSON file.\n",
    "output_file = \"asr_output.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(transcriptions, f, indent=4)\n",
    "\n",
    "print(f\"Transcriptions have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
